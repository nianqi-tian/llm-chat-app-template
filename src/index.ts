/**
 * LLM Chat Application Template (æœ€ç»ˆä¿®æ­£ V3ï¼šå¼ºåˆ¶æµå¼ä¼ è¾“)
 */
import { v4 as uuidv4 } from 'uuid'; 
import { Env, ChatMessage, ConversationHistory, Message } from "./types";

// ... (å…¨å±€çŠ¶æ€ã€å¸¸é‡ã€readHistory, saveConversation, handleGetHistory, handlePostCancel ä¿æŒä¸å˜) ...
// --- è¿è¡Œæ—¶å¯å˜é…ç½® & å…¨å±€çŠ¶æ€ ---
const activeControllers = new Map<string, AbortController>(); 

// é»˜è®¤æ¨¡å‹ä¸å‚æ•°ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆå‚è§ wrangler.jsonc varsï¼‰
const DEFAULT_MODEL_ID = "@cf/meta/llama-3.3-70b-instruct-fp8-fast";
const DEFAULT_TEMPERATURE = 0.3;
const DEFAULT_MAX_OUTPUT_TOKENS = 1024;
const DEFAULT_MAX_CONTEXT_TOKENS = 4000;

const BASE_SYSTEM_PROMPT = "You are a helpful, friendly assistant. Provide concise and accurate responses.";

async function readHistory(env: Env, conversationId: string): Promise<ConversationHistory> {
    if (!conversationId) return [];
    try {
        const historyJson = await env.CHAT_HISTORY.get(conversationId);
        if (historyJson) return JSON.parse(historyJson) as ConversationHistory;
    } catch (error) {
        console.error(`[KV ERROR] Read/Parse failed for ${conversationId}:`, error); 
    }
    return []; 
}

async function saveConversation(env: Env, conversationId: string, history: ConversationHistory): Promise<void> {
    try {
        const historyJsonString = JSON.stringify(history);
        await env.CHAT_HISTORY.put(conversationId, historyJsonString);
    } catch (error) {
        console.error(`[KV ERROR] Write failed for ${conversationId}:`, error);
    }
}

async function handleGetHistory(request: Request, env: Env): Promise<Response> {
    const url = new URL(request.url);
    const conversationId = url.searchParams.get('id'); 
    if (!conversationId) return new Response(JSON.stringify({ error: "è¯·æ±‚ç¼ºå°‘ conversationId å‚æ•°" }), { status: 400, headers: { 'Content-Type': 'application/json' }});
    try {
        const history = await readHistory(env, conversationId);
        return new Response(JSON.stringify({ conversationId, history }), { headers: { 'Content-Type': 'application/json' }, status: 200, });
    } catch (error) {
        console.error(`[API ERROR] Failed to retrieve history for ${conversationId}:`, error);
        return new Response(JSON.stringify({ error: "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œæ— æ³•åŠ è½½å†å²è®°å½•" }), { status: 500, headers: { 'Content-Type': 'application/json' }});
    }
}

async function handlePostCancel(request: Request): Promise<Response> {
    const url = new URL(request.url);
    const pathSegments = url.pathname.split('/');
    const conversationId = pathSegments[pathSegments.length - 2]; 

    const controller = activeControllers.get(conversationId);
    if (controller) {
        controller.abort(); 
        activeControllers.delete(conversationId);
        return new Response(JSON.stringify({ status: 'cancelled' }), { status: 200, headers: { 'Content-Type': 'application/json' },});
    } else {
        return new Response(JSON.stringify({ status: 'no active request found' }), { status: 404, headers: { 'Content-Type': 'application/json' },});
    }
}


// --- ç®€å• Token ä¼°ç®—ä¸è£å‰ª ---
function estimateTokensFromMessages(messages: ChatMessage[]): number {
    // éç²¾ç¡®ä¼°ç®—ï¼šå‡è®¾ 4 å­—ç¬¦ â‰ˆ 1 token
    const totalChars = messages.reduce((sum, m) => sum + m.content.length, 0);
    return Math.ceil(totalChars / 4);
}

function trimMessagesToTokenLimit(messages: ChatMessage[], maxTokens: number): ChatMessage[] {
    // ä¿ç•™æœ€åçš„å¯¹è¯è½®æ¬¡ä¸ system æç¤ºï¼Œä¸¢å¼ƒæœ€æ—©çš„ user/assistant å†…å®¹
    if (estimateTokensFromMessages(messages) <= maxTokens) return messages;

    const systemMessages = messages.filter(m => m.role === 'system');
    const nonSystem = messages.filter(m => m.role !== 'system');

    while (nonSystem.length > 0 && estimateTokensFromMessages([...systemMessages, ...nonSystem]) > maxTokens) {
        nonSystem.shift(); // ä¸¢å¼ƒæœ€æ—©çš„ä¸€æ¡
    }
    return [...systemMessages, ...nonSystem];
}


// --- æ ¸å¿ƒèŠå¤©é€»è¾‘ï¼šæœ€ç»ˆä¿®æ­£ ---
async function handlePostChat(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
    try {
        const body = (await request.json()) as {
            messages?: ChatMessage[];
            conversationId?: string | null;
            options?: {
                model?: string;
                temperature?: number;
                max_tokens?: number;
                webSearchEnabled?: boolean;
            };
        };

        const frontendMessages = body.messages ?? [];
        const oldConversationId = body.conversationId;

        if (!Array.isArray(frontendMessages) || frontendMessages.length === 0) {
            return new Response(JSON.stringify({ error: "è¯·æ±‚ä½“ä¸­ç¼ºå°‘æœ‰æ•ˆçš„ messages æ•°ç»„" }), {
                status: 400,
                headers: { "content-type": "application/json" },
            });
        }

        const lastMsg = frontendMessages[frontendMessages.length - 1];
        if (!lastMsg || lastMsg.role !== "user" || typeof lastMsg.content !== "string" || !lastMsg.content.trim()) {
            return new Response(JSON.stringify({ error: "æœ€åä¸€æ¡æ¶ˆæ¯å¿…é¡»æ˜¯éç©ºçš„ç”¨æˆ·æ¶ˆæ¯" }), {
                status: 400,
                headers: { "content-type": "application/json" },
            });
        }

        // ğŸš¨ ä¿®æ­£ ID é€»è¾‘ï¼šç¡®ä¿æ—§ ID ä¸º 'null' æˆ– undefined/null æ—¶ç”Ÿæˆæ–° ID
        const conversationId = oldConversationId && oldConversationId !== 'null' ? oldConversationId : uuidv4(); 
        
        const controller = new AbortController();
        activeControllers.set(conversationId, controller);
        
        const history = await readHistory(env, conversationId);
        const userMessageContent = lastMsg.content.trim();
        
        const userMessage: Message = { role: 'user', content: userMessageContent, timestamp: Date.now() };

        let messagesForAI: ChatMessage[] = history.map(m => ({ role: m.role, content: m.content } as ChatMessage));
        
        const webSearchEnabled = body.options?.webSearchEnabled === true;
        let systemPrompt = BASE_SYSTEM_PROMPT;
        if (webSearchEnabled) {
            systemPrompt += " You may use web search or external knowledge if available to provide up-to-date information.";
        }

        if (!messagesForAI.some((msg) => msg.role === "system")) {
            messagesForAI.unshift({ role: "system", content: systemPrompt });
        } else {
            // å¦‚æœå·²ç»æœ‰ system æ¶ˆæ¯ï¼Œä¿ç•™æœ€åä¸€æ¡ï¼Œä½†è¿½åŠ  webSearch è¯´æ˜
            messagesForAI = messagesForAI.map((m, idx) =>
                m.role === "system" && idx === messagesForAI.findLastIndex(mm => mm.role === "system")
                    ? { ...m, content: m.content + (webSearchEnabled ? "\n\n(å½“å‰ä¼šè¯å…è®¸ä½¿ç”¨ Web æœç´¢ä»¥æä¾›å°½é‡æ–°çš„ä¿¡æ¯ã€‚)" : "") }
                    : m
            );
        }
        messagesForAI.push(userMessage as ChatMessage);

        // Token ä¼°ç®—ä¸è£å‰ªï¼ˆåŒ…å«ç”¨æˆ·æ–°æ¶ˆæ¯ï¼‰
        const configuredMaxContextTokens = Number(env.MAX_CONTEXT_TOKENS ?? DEFAULT_MAX_CONTEXT_TOKENS);
        messagesForAI = trimMessagesToTokenLimit(messagesForAI, configuredMaxContextTokens);

        // 4. è°ƒç”¨ Workers AIï¼ˆå¯é…ç½®çš„æ¨¡å‹ä¸å‚æ•°ï¼‰
        const modelId = body.options?.model || env.MODEL_ID || DEFAULT_MODEL_ID;
        const temperature =
            typeof body.options?.temperature === "number"
                ? body.options.temperature
                : Number(env.TEMPERATURE ?? DEFAULT_TEMPERATURE);
        const maxOutputTokens =
            typeof body.options?.max_tokens === "number"
                ? body.options.max_tokens
                : Number(env.MAX_OUTPUT_TOKENS ?? DEFAULT_MAX_OUTPUT_TOKENS);

        const requestStart = Date.now();

        // ç®€å•çš„ä¸€æ¬¡é‡è¯•æœºåˆ¶ï¼šå¦‚æœç¬¬ä¸€æ¬¡è°ƒç”¨å‡ºç°ç½‘ç»œç±»é”™è¯¯ï¼Œå†å°è¯•ä¸€æ¬¡
        async function callModelOnce(): Promise<Response> {
            return (await env.AI.run(
                modelId,
                { messages: messagesForAI, max_tokens: maxOutputTokens, temperature },
                { signal: controller.signal, returnRawResponse: true },
            )) as unknown as Response;
        }

        let llmResponse: Response;
        try {
            llmResponse = await callModelOnce();
        } catch (err) {
            console.warn("[LLM] ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥ï¼Œå°è¯•é‡è¯•ä¸€æ¬¡:", err);
            llmResponse = await callModelOnce();
        }

        if (!llmResponse.ok) {
            console.error("Workers AI åŸå§‹è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç :", llmResponse.status);
            return new Response(JSON.stringify({ error: "LLM Provider Error" }), {
                status: 502,
                headers: { 'Content-Type': 'application/json' },
            });
        }

        // 5. æå–æµå¹¶è®¾ç½®æŒä¹…åŒ–é€»è¾‘
        let fullAiResponseContent = '';
        let isInterrupted = false;
        
        // ä½¿ç”¨ llmResponse.body.pipeThrough() åˆ›å»ºä¸€ä¸ªæ–°çš„æµï¼Œç¡®ä¿å®ƒèƒ½è¢«æ­£ç¡®è¯†åˆ«ä¸ºæµå¼ä¼ è¾“
        // ğŸš¨ ä¿®æ­£ï¼šä½¿ç”¨ pipeTo() æ¥æ”¶é›†å†…å®¹ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„å“åº”æµ
        const [streamForSaving, streamForResponse] = llmResponse.body!.tee();

        ctx.waitUntil((async () => {
            try {
                const reader = streamForSaving.getReader();
                const decoder = new TextDecoder();
                
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    
                    fullAiResponseContent += decoder.decode(value);
                }
            } catch (error) {
                if (error instanceof DOMException && error.name === 'AbortError') {
                    isInterrupted = true;
                } else {
                    console.error("AI.run æµæ”¶é›†é”™è¯¯:", error);
                }
            } finally {
                activeControllers.delete(conversationId); 
                
                const aiMessage: Message = {
                    role: 'assistant',
                    content: fullAiResponseContent,
                    timestamp: Date.now(),
                    interrupted: isInterrupted,
                };
                
                const updatedHistory = [...history, userMessage, aiMessage];
                await saveConversation(env, conversationId, updatedHistory); 

                const elapsedMs = Date.now() - requestStart;
                const promptTokens = estimateTokensFromMessages(messagesForAI);
                const completionTokens = Math.ceil(fullAiResponseContent.length / 4);
                console.log(
                    `[METRICS] conversationId=${conversationId} model=${modelId} webSearch=${webSearchEnabled} ` +
                    `promptTokens=${promptTokens} completionTokens=${completionTokens} durationMs=${elapsedMs}`
                );
            }
        })());

        // 7. è¿”å›æµå¼å“åº”
        const response = new Response(streamForResponse, {
            status: llmResponse.status,
            headers: {
                ...llmResponse.headers,
                // ç¡®ä¿ Content-Type è‡³å°‘æ˜¯ text/plain æˆ– application/octet-streamï¼Œ
                // æµè§ˆå™¨é€šå¸¸ä¼šå°†å…¶è§†ä¸ºæµå¼ä¼ è¾“
                'Content-Type': 'text/plain', 
                'X-Conversation-ID': conversationId, 
            },
        });
        return response;

    } catch (error) {
        console.error("Error processing chat request:", error);
        return new Response(JSON.stringify({ error: "Failed to process request" }), {
            status: 500,
            headers: { "content-type": "application/json" },
        });
    }
}


// --- ç®€å•å†…å­˜çº§ Rate Limitingï¼ˆå• Worker å®ä¾‹çº§åˆ«ï¼Œé˜²æ­¢æ»¥ç”¨ï¼‰ ---
const rateLimitMap = new Map<string, { windowStart: number; count: number }>();

function getClientKey(request: Request): string {
    // åœ¨ Workers ä¸­å¯ä»¥é€šè¿‡ request.headers.get("CF-Connecting-IP") è·å–ç”¨æˆ· IP
    return request.headers.get("CF-Connecting-IP") || "unknown";
}

function checkRateLimit(request: Request, env: Env): { allowed: boolean; retryAfterSec?: number } {
    const key = getClientKey(request);
    const now = Date.now();

    const windowSec = Number(env.RATE_LIMIT_WINDOW_SEC ?? 60);
    const maxRequests = Number(env.RATE_LIMIT_MAX_REQUESTS ?? 30);
    const windowMs = windowSec * 1000;

    const record = rateLimitMap.get(key);

    if (!record || now - record.windowStart >= windowMs) {
        rateLimitMap.set(key, { windowStart: now, count: 1 });
        return { allowed: true };
    }

    if (record.count < maxRequests) {
        record.count += 1;
        return { allowed: true };
    }

    const retryAfterSec = Math.ceil((record.windowStart + windowMs - now) / 1000);
    return { allowed: false, retryAfterSec };
}


export default {
    async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
        const url = new URL(request.url);

        if (request.method === "OPTIONS") {
            const headers = {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Conversation-ID',
                'Access-Control-Max-Age': '86400',
            };
            return new Response(null, { status: 204, headers });
        }

        // é™æ€èµ„æº
        if (url.pathname === "/" || !url.pathname.startsWith("/api/")) {
            return env.ASSETS.fetch(request);
        }

        // å†å²è®°å½•è¯»å–
        if (url.pathname.startsWith("/api/history") && request.method === "GET") {
            return handleGetHistory(request, env);
        }

        // å–æ¶ˆç”Ÿæˆ
        if (request.method === 'POST' && url.pathname.match(/\/api\/chat\/[^/]+\/cancel$/)) {
            return handlePostCancel(request);
        }

        // Rate limit ä¸»è¦é’ˆå¯¹èŠå¤©æ¥å£
        if (url.pathname === "/api/chat" && request.method === "POST") {
            const rl = checkRateLimit(request, env);
            if (!rl.allowed) {
                return new Response(JSON.stringify({ error: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•ã€‚" }), {
                    status: 429,
                    headers: {
                        "content-type": "application/json",
                        "Retry-After": String(rl.retryAfterSec ?? 60),
                    },
                });
            }
            return handlePostChat(request, env, ctx); 
        }

        return new Response("Not found", { status: 404 });
    },
} satisfies ExportedHandler<Env>;