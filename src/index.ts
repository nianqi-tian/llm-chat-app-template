/**
 * LLM Chat Application Template (æœ€ç»ˆä¿®æ­£ V3ï¼šå¼ºåˆ¶æµå¼ä¼ è¾“)
 */
import { v4 as uuidv4 } from 'uuid'; 
import { Env, ChatMessage, ConversationHistory, Message } from "./types";

// ... (å…¨å±€çŠ¶æ€ã€å¸¸é‡ã€readHistory, saveConversation, handleGetHistory, handlePostCancel ä¿æŒä¸å˜) ...
const activeControllers = new Map<string, AbortController>(); 
const MODEL_ID = "@cf/meta/llama-3.3-70b-instruct-fp8-fast"; 
const SYSTEM_PROMPT = "You are a helpful, friendly assistant. Provide concise and accurate responses.";

async function readHistory(env: Env, conversationId: string): Promise<ConversationHistory> {
    if (!conversationId) return [];
    try {
        const historyJson = await env.CHAT_HISTORY.get(conversationId);
        if (historyJson) return JSON.parse(historyJson) as ConversationHistory;
    } catch (error) {
        console.error(`[KV ERROR] Read/Parse failed for ${conversationId}:`, error); 
    }
    return []; 
}

async function saveConversation(env: Env, conversationId: string, history: ConversationHistory): Promise<void> {
    try {
        const historyJsonString = JSON.stringify(history);
        await env.CHAT_HISTORY.put(conversationId, historyJsonString);
    } catch (error) {
        console.error(`[KV ERROR] Write failed for ${conversationId}:`, error);
    }
}

async function handleGetHistory(request: Request, env: Env): Promise<Response> {
    const url = new URL(request.url);
    const conversationId = url.searchParams.get('id'); 
    if (!conversationId) return new Response(JSON.stringify({ error: "è¯·æ±‚ç¼ºå°‘ conversationId å‚æ•°" }), { status: 400, headers: { 'Content-Type': 'application/json' }});
    try {
        const history = await readHistory(env, conversationId);
        return new Response(JSON.stringify({ conversationId, history }), { headers: { 'Content-Type': 'application/json' }, status: 200, });
    } catch (error) {
        console.error(`[API ERROR] Failed to retrieve history for ${conversationId}:`, error);
        return new Response(JSON.stringify({ error: "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œæ— æ³•åŠ è½½å†å²è®°å½•" }), { status: 500, headers: { 'Content-Type': 'application/json' }});
    }
}

async function handlePostCancel(request: Request): Promise<Response> {
    const url = new URL(request.url);
    const pathSegments = url.pathname.split('/');
    const conversationId = pathSegments[pathSegments.length - 2]; 

    const controller = activeControllers.get(conversationId);
    if (controller) {
        controller.abort(); 
        activeControllers.delete(conversationId);
        return new Response(JSON.stringify({ status: 'cancelled' }), { status: 200, headers: { 'Content-Type': 'application/json' },});
    } else {
        return new Response(JSON.stringify({ status: 'no active request found' }), { status: 404, headers: { 'Content-Type': 'application/json' },});
    }
}


// --- æ ¸å¿ƒèŠå¤©é€»è¾‘ï¼šæœ€ç»ˆä¿®æ­£ ---
async function handlePostChat(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
    try {
        const { messages: frontendMessages = [], conversationId: oldConversationId } = (await request.json()) as {
            messages: ChatMessage[]; 
            conversationId?: string; 
        };

        // ğŸš¨ ä¿®æ­£ ID é€»è¾‘ï¼šç¡®ä¿æ—§ ID ä¸º 'null' æˆ– undefined/null æ—¶ç”Ÿæˆæ–° ID
        const conversationId = oldConversationId && oldConversationId !== 'null' ? oldConversationId : uuidv4(); 
        
        const controller = new AbortController();
        activeControllers.set(conversationId, controller);
        
        const history = await readHistory(env, conversationId);
        const userMessageContent = frontendMessages[frontendMessages.length - 1].content;
        
        const userMessage: Message = { role: 'user', content: userMessageContent, timestamp: Date.now() };

        let messagesForAI: ChatMessage[] = history.map(m => ({ role: m.role, content: m.content } as ChatMessage));
        
        if (!messagesForAI.some((msg) => msg.role === "system")) {
            messagesForAI.unshift({ role: "system", content: SYSTEM_PROMPT });
        }
        messagesForAI.push(userMessage as ChatMessage);

        // 4. è°ƒç”¨ Workers AI
        const llmResponse = (await env.AI.run(
            MODEL_ID,
            { messages: messagesForAI, max_tokens: 1024 },
            { signal: controller.signal, returnRawResponse: true },
        )) as unknown as Response;

        if (!llmResponse.ok) {
            console.error("Workers AI åŸå§‹è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç :", llmResponse.status);
            return new Response(JSON.stringify({ error: "LLM Provider Error" }), { status: 502, headers: { 'Content-Type': 'application/json' }});
        }

        // 5. æå–æµå¹¶è®¾ç½®æŒä¹…åŒ–é€»è¾‘
        let fullAiResponseContent = '';
        let isInterrupted = false;
        
        // ä½¿ç”¨ llmResponse.body.pipeThrough() åˆ›å»ºä¸€ä¸ªæ–°çš„æµï¼Œç¡®ä¿å®ƒèƒ½è¢«æ­£ç¡®è¯†åˆ«ä¸ºæµå¼ä¼ è¾“
        // ğŸš¨ ä¿®æ­£ï¼šä½¿ç”¨ pipeTo() æ¥æ”¶é›†å†…å®¹ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„å“åº”æµ
        const [streamForSaving, streamForResponse] = llmResponse.body!.tee();

        ctx.waitUntil((async () => {
            try {
                const reader = streamForSaving.getReader();
                const decoder = new TextDecoder();
                
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    
                    fullAiResponseContent += decoder.decode(value);
                }
            } catch (error) {
                if (error instanceof DOMException && error.name === 'AbortError') {
                    isInterrupted = true;
                } else {
                    console.error("AI.run æµæ”¶é›†é”™è¯¯:", error);
                }
            } finally {
                activeControllers.delete(conversationId); 
                
                const aiMessage: Message = { role: 'assistant', content: fullAiResponseContent, timestamp: Date.now(), interrupted: isInterrupted };
                
                const updatedHistory = [...history, userMessage, aiMessage];
                await saveConversation(env, conversationId, updatedHistory); 
                console.log(`å¯¹è¯ ${conversationId} ä¿å­˜å®Œæˆã€‚`);
            }
        })());

        // 7. è¿”å›æµå¼å“åº”
        const response = new Response(streamForResponse, {
            status: llmResponse.status,
            headers: {
                ...llmResponse.headers,
                // ç¡®ä¿ Content-Type è‡³å°‘æ˜¯ text/plain æˆ– application/octet-streamï¼Œ
                // æµè§ˆå™¨é€šå¸¸ä¼šå°†å…¶è§†ä¸ºæµå¼ä¼ è¾“
                'Content-Type': 'text/plain', 
                'X-Conversation-ID': conversationId, 
            },
        });
        return response;

    } catch (error) {
        console.error("Error processing chat request:", error);
        return new Response(JSON.stringify({ error: "Failed to process request" }), { status: 500, headers: { "content-type": "application/json" }});
    }
}


// ... (export default { fetch(...) è·¯ç”±é€»è¾‘ä¿æŒä¸å˜) ...
export default {
    async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
        const url = new URL(request.url);

        if (request.method === "OPTIONS") {
             const headers = {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Conversation-ID',
                'Access-Control-Max-Age': '86400',
             };
             return new Response(null, { status: 204, headers });
        }

        if (url.pathname === "/" || !url.pathname.startsWith("/api/")) {
            return env.ASSETS.fetch(request);
        }

        if (url.pathname.startsWith("/api/history") && request.method === "GET") {
            return handleGetHistory(request, env);
        }

        if (request.method === 'POST' && url.pathname.match(/\/api\/chat\/[^/]+\/cancel$/)) {
             return handlePostCancel(request);
        }

        if (url.pathname === "/api/chat" && request.method === "POST") {
            return handlePostChat(request, env, ctx); 
        }

        return new Response("Not found", { status: 404 });
    },
} satisfies ExportedHandler<Env>;